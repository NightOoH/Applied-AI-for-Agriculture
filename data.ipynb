{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate removal complete.\n",
      "Balanced distribution complete.\n",
      "Dataset split complete.\n"
     ]
    }
   ],
   "source": [
    "import os  # Quản lý thao tác với hệ thống tệp\n",
    "import random  # Sinh số ngẫu nhiên\n",
    "import shutil  # Quản lý việc sao chép, di chuyển tệp\n",
    "from collections import defaultdict  # Sử dụng từ điển với giá trị mặc định\n",
    "from PIL import Image  # Xử lý hình ảnh\n",
    "import numpy as np  # Xử lý mảng và tính toán số học\n",
    "import pandas as pd  # Quản lý dữ liệu dạng bảng (DataFrame)\n",
    "\n",
    "# 1. Tính Average Hash\n",
    "def average_hash(image_path, hash_size=16):\n",
    "    img = Image.open(image_path).convert(\"L\")  # Chuyển sang grayscale\n",
    "    img = img.resize((hash_size, hash_size), Image.Resampling.LANCZOS)  # Resize ảnh\n",
    "    pixels = np.array(img)\n",
    "    avg = pixels.mean()\n",
    "    hash_bits = (pixels >= avg).astype(int).flatten()  # Chuyển thành giá trị nhị phân\n",
    "    hash_string = ''.join(hash_bits.astype(str))  # Chuyển mảng nhị phân thành chuỗi\n",
    "    return hash_string\n",
    "\n",
    "#2. Remove duplicate images and labels with hash saving\n",
    "def remove_duplicates(image_folder, label_folder, duplicate_image_folder, label_duplicate_folder, excel_file):\n",
    "    os.makedirs(duplicate_image_folder, exist_ok=True)\n",
    "    os.makedirs(label_duplicate_folder, exist_ok=True)\n",
    "\n",
    "    # Read saved hash data from Excel\n",
    "    if os.path.exists(excel_file):\n",
    "        existing_data = pd.read_excel(excel_file)\n",
    "        file_hashes = dict(zip(existing_data[\"Hash\"], existing_data[\"Filename\"]))\n",
    "    else:\n",
    "        file_hashes = {}\n",
    "\n",
    "    image_data = []\n",
    "\n",
    "    for root, _, files in os.walk(image_folder):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                file_hash = average_hash(file_path)\n",
    "\n",
    "                if file_hash in file_hashes:\n",
    "                    shutil.move(file_path, os.path.join(duplicate_image_folder, filename))\n",
    "                    label_file = os.path.splitext(filename)[0] + '.txt'\n",
    "                    label_path = os.path.join(label_folder, label_file)\n",
    "                    if os.path.exists(label_path):\n",
    "                        shutil.move(label_path, os.path.join(label_duplicate_folder, label_file))\n",
    "                else:\n",
    "                    file_hashes[file_hash] = filename\n",
    "                    image_data.append([filename, file_hash])\n",
    "\n",
    "    # Save the new hash information to Excel\n",
    "    if image_data:\n",
    "        new_df = pd.DataFrame(image_data, columns=[\"Filename\", \"Hash\"])\n",
    "        if os.path.exists(excel_file):\n",
    "            existing_df = pd.read_excel(excel_file)\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "            combined_df.to_excel(excel_file, index=False)\n",
    "        else:\n",
    "            new_df.to_excel(excel_file, index=False)\n",
    "\n",
    "    print(\"Duplicate removal complete.\")\n",
    "\n",
    "#3. Balanced label data distribution across classes\n",
    "def distribute_labels_balanced(dataset_path, output_path, target):\n",
    "    images_path = os.path.join(dataset_path, \"images\")\n",
    "    labels_path = os.path.join(dataset_path, \"labels\")\n",
    "    output_images_path = os.path.join(output_path, \"images\")\n",
    "    output_labels_path = os.path.join(output_path, \"labels\")\n",
    "\n",
    "    os.makedirs(output_images_path, exist_ok=True)\n",
    "    os.makedirs(output_labels_path, exist_ok=True)\n",
    "\n",
    "    final_label_count = defaultdict(int)\n",
    "\n",
    "    # Read all `.txt` files in `labels_path` directory\n",
    "    for root, _, files in os.walk(labels_path):\n",
    "        for label_file in files:\n",
    "            if label_file.endswith(\".txt\"):\n",
    "                label_file_path = os.path.join(root, label_file)\n",
    "\n",
    "                class_counts = defaultdict(int)\n",
    "                with open(label_file_path, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        class_id = line.split()[0]\n",
    "                        class_counts[class_id] += 1\n",
    "\n",
    "                # Make sure the corresponding image files exist before copying\n",
    "                image_file = label_file.replace(\".txt\", \".jpg\")\n",
    "                image_file_path = os.path.join(images_path, image_file)\n",
    "                if os.path.exists(image_file_path):\n",
    "                    shutil.copy(label_file_path, os.path.join(output_labels_path, label_file))\n",
    "                    shutil.copy(image_file_path, os.path.join(output_images_path, image_file))\n",
    "                    for class_id, count in class_counts.items():\n",
    "                        final_label_count[class_id] += count\n",
    "                else:\n",
    "                    print(f\"Warning: Image file {image_file} does not exist for label file {label_file}\")\n",
    "\n",
    "    print(\"Balanced distribution complete.\")\n",
    "    for class_id, count in sorted(final_label_count.items()):\n",
    "        print(f\"Class {class_id}: {count}\")\n",
    "\n",
    "# 4. Split data into train, test, val\n",
    "def split_dataset(dataset_path, output_path, split_ratios):\n",
    "    images_path = os.path.join(dataset_path, \"images\")\n",
    "    labels_path = os.path.join(dataset_path, \"labels\")\n",
    "\n",
    "    for split in [\"train\", \"test\", \"val\"]:\n",
    "        os.makedirs(os.path.join(output_path, split, \"images\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_path, split, \"labels\"), exist_ok=True)\n",
    "\n",
    "    label_data = defaultdict(list)\n",
    "\n",
    "    # Read all `.txt` files in `labels_path` directory\n",
    "    for root, _, files in os.walk(labels_path):\n",
    "        for label_file in files:\n",
    "            if label_file.endswith(\".txt\"):\n",
    "                with open(os.path.join(root, label_file), \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                class_ids = {line.split()[0] for line in lines}\n",
    "                for class_id in class_ids:\n",
    "                    label_data[class_id].append(label_file)\n",
    "\n",
    "    # Distribute data into train, test, val\n",
    "    split_data = {\"train\": [], \"test\": [], \"val\": []}\n",
    "    used_files = set()\n",
    "\n",
    "    for class_id, files in label_data.items():\n",
    "        random.shuffle(files)\n",
    "        n_total = len(files)\n",
    "        n_train = int(n_total * split_ratios[\"train\"])\n",
    "        n_test = int(n_total * split_ratios[\"test\"])\n",
    "\n",
    "        split_data[\"train\"].extend(files[:n_train])\n",
    "        split_data[\"test\"].extend(files[n_train:n_train + n_test])\n",
    "        split_data[\"val\"].extend(files[n_train + n_test:])\n",
    "\n",
    "    for split, files in split_data.items():\n",
    "        for label_file in files:\n",
    "            shutil.copy(os.path.join(labels_path, label_file), os.path.join(output_path, split, \"labels\", label_file))\n",
    "            image_file = label_file.replace(\".txt\", \".jpg\")\n",
    "            shutil.copy(os.path.join(images_path, image_file), os.path.join(output_path, split, \"images\", image_file))\n",
    "\n",
    "    print(\"Dataset split complete.\")\n",
    "\n",
    "# Main Function\n",
    "if __name__ == \"__main__\":\n",
    "    remove_duplicates(\n",
    "        \"Dataset/tomato/images\",\n",
    "        \"Dataset/tomato/labels\",\n",
    "        \"Dataset/tomato/images_dup\",\n",
    "        \"Dataset/tomato/labels_dup\",\n",
    "        \"Dataset/tomato/image_descriptors1.xlsx\"\n",
    "    )\n",
    "    distribute_labels_balanced(\"Dataset/tomato\", \"Dataset/tomato/balanced\", target=7000)\n",
    "    split_dataset(\"Dataset/tomato/balanced\", \"Dataset/tomato/split\", {\"train\": 0.8, \"test\": 0.1, \"val\": 0.1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Quản lý thao tác với hệ thống tệp\n",
    "import random  # Sinh số ngẫu nhiên\n",
    "import shutil  # Quản lý việc sao chép, di chuyển tệp\n",
    "from collections import defaultdict  # Sử dụng từ điển với giá trị mặc định\n",
    "from PIL import Image  # Xử lý hình ảnh\n",
    "import numpy as np  # Xử lý mảng và tính toán số học\n",
    "import pandas as pd  # Quản lý dữ liệu dạng bảng (DataFrame)\n",
    "import matplotlib.pyplot as plt  # Hiển thị ảnh và biểu đồ\n",
    "\n",
    "# === 1. Tính Average Hash ===\n",
    "def average_hash(image_path, hash_size=16):\n",
    "    \"\"\"Tính Average Hash (AHash) của ảnh.\"\"\"\n",
    "    img = Image.open(image_path).convert(\"L\")  # Chuyển sang grayscale\n",
    "    img = img.resize((hash_size, hash_size), Image.Resampling.LANCZOS)  # Resize ảnh\n",
    "    pixels = np.array(img)\n",
    "    avg = pixels.mean()\n",
    "    hash_bits = (pixels >= avg).astype(int).flatten()  # Chuyển thành giá trị nhị phân\n",
    "    hash_string = ''.join(hash_bits.astype(str))  # Chuyển mảng nhị phân thành chuỗi\n",
    "    return hash_string\n",
    "\n",
    "# === 2. So sánh ảnh dựa trên hash ===\n",
    "def compare_images(hash1, hash2):\n",
    "    \"\"\"So sánh hai hash ảnh dựa trên Hamming distance.\"\"\"\n",
    "    if len(hash1) != len(hash2):\n",
    "        raise ValueError(\"Hash length mismatch!\")\n",
    "    return sum(c1 != c2 for c1, c2 in zip(hash1, hash2))  # Đếm khác biệt\n",
    "\n",
    "# === 3. Loại bỏ ảnh và nhãn trùng lặp với lưu hash ===\n",
    "def remove_duplicates(image_folder, label_folder, duplicate_image_folder, duplicate_label_folder, excel_file):\n",
    "    os.makedirs(duplicate_image_folder, exist_ok=True)\n",
    "    os.makedirs(duplicate_label_folder, exist_ok=True)\n",
    "\n",
    "    # Đọc dữ liệu hash đã lưu từ Excel\n",
    "    if os.path.exists(excel_file):\n",
    "        existing_data = pd.read_excel(excel_file)\n",
    "        file_hashes = dict(zip(existing_data[\"Hash\"], existing_data[\"Filename\"]))\n",
    "    else:\n",
    "        file_hashes = {}\n",
    "\n",
    "    image_data = []  # Danh sách lưu trữ hash mới\n",
    "\n",
    "    for root, _, files in os.walk(image_folder):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                file_hash = average_hash(file_path)  # Tính Average Hash\n",
    "\n",
    "                is_duplicate = False\n",
    "                for existing_hash, existing_file in file_hashes.items():\n",
    "                    if compare_images(file_hash, existing_hash) <= 5:  # Ngưỡng tương tự (Hamming distance)\n",
    "                        print(f\"Duplicate image found: {filename} (Similar to {existing_file})\")\n",
    "                        # Di chuyển ảnh trùng\n",
    "                        shutil.move(file_path, os.path.join(duplicate_image_folder, filename))\n",
    "\n",
    "                        # Kiểm tra và di chuyển nhãn\n",
    "                        label_file = os.path.splitext(filename)[0] + '.txt'\n",
    "                        label_path = os.path.join(label_folder, label_file)\n",
    "                        if os.path.exists(label_path):\n",
    "                            shutil.move(label_path, os.path.join(duplicate_label_folder, label_file))\n",
    "                            print(f\"Moved label {label_file} to {duplicate_label_folder}\")\n",
    "                        else:\n",
    "                            print(f\"Label file not found for {filename}\")\n",
    "\n",
    "                        is_duplicate = True\n",
    "                        break\n",
    "\n",
    "                if not is_duplicate:\n",
    "                    file_hashes[file_hash] = filename\n",
    "                    image_data.append([filename, file_hash])\n",
    "\n",
    "    # Lưu thông tin hash mới vào Excel\n",
    "    if image_data:\n",
    "        new_df = pd.DataFrame(image_data, columns=[\"Filename\", \"Hash\"])\n",
    "        if os.path.exists(excel_file):\n",
    "            existing_df = pd.read_excel(excel_file)\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "            combined_df.to_excel(excel_file, index=False)\n",
    "        else:\n",
    "            new_df.to_excel(excel_file, index=False)\n",
    "\n",
    "    print(\"Duplicate removal complete.\")\n",
    "\n",
    "# === Main Function ===\n",
    "if __name__ == \"__main__\":\n",
    "    remove_duplicates(\n",
    "        \"straw.v1i.yolov9/data/images\",\n",
    "        \"straw.v1i.yolov9/data/labels\",\n",
    "        \"straw.v1i.yolov9/data/images_dup6\",\n",
    "        \"straw.v1i.yolov9/data/labels_dup6\",\n",
    "        \"straw.v1i.yolov9/data/image_hashes6.xlsx\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
